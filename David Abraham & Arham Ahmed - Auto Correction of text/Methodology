Methodology:



Up till now most of our work included gathering the necessary data required to train and test our model.


It's time to integrate this data into a working machine model.


In order to develop the autocorrection model, an algorithm that lays the groundwork for the infrastructure of the cooded program is required.



There exist a wide range of algorithms that can help us effectively reach our goal, such as:


1. Logistic regression 

2. Decision tree

3. Random forest

4. Support Vector Machine

5. K nearest neighbour

6. Naive Bays





Logistic Regression:

-Logistic regression is a statistical analysis method to predict a binary outcome, such as yes or no, based on prior observations of a data set.

- A logistic regression model predicts a dependent data variable by analyzing the relationship between one or more existing independent variables. For example, a logistic regression could be used to predict whether a political candidate will win or lose an election or whether a high school student will be admitted or not to a particular college. These binary outcomes allow straightforward decisions between two alternatives.


Decision  tree:

- Decision tree builds tree branches in a hierarchy approach and each branch can be considered as an if-else statement. The branches develop by partitioning the dataset into subsets based on most important features. Final classification happens at the leaves of the decision tree.

- To elaborate,a decision tree is a graph that uses a branching method to illustrate every possible output for a specific input


Random forest:

- As the name suggest, random forest is a collection of decision trees. It is a common type of ensemble methods which aggregate results from multiple predictors. Random forest additionally utilizes bagging technique that allows each tree trained on a random sampling of original dataset and takes the majority vote from trees. Compared to decision tree, it has better generalization but less interpretable, because of more layers added to the model.


Support Vector Machine:

- Support vector machine finds the best way to classify the data based on the position in relation to a border between positive class and negative class. This border is known as the hyperplane which maximize the distance between data points from different classes. Similar to decision tree and random forest, support vector machine can be used in both classification and regression, SVC (support vector classifier) is for classification problem.

- A support vector machine is also known as a support vector network (SVN).

K nearest neighbour:

What is meant by K nearest neighbor?

- A k-nearest-neighbor is a data classification algorithm that attempts to determine what group a data point is in by looking at the data points around it.

- KNN is a non-parametric method used for classification

Naive Bayes:

- Naive Bayes is based on an approach to calculate conditional probability based on prior knowledge, and the naive assumption that each feature is independent to each other.

- The biggest advantage of Naive Bayes is that, while most machine learning algorithms rely on large amount of training data, it performs relatively well even when the training data size is small. Gaussian Naive Bayes is a type of Naive Bayes classifier that follows the normal distribution. 




   After reviewing the purposes and uses of these six different algorithms, we concluded that the "Naive Bayes" deemed the most appropriate for our
autocorrection model. The unique approach of this algorithm best suited the goals that our model intends to achieve. Hnece, the following Steps will
be based on the "Naive Bayes".  

Steps:

Here we will demonstrate how we will lay out our procedures




First we would need to import the neccessary librabies:
 
-Pandas
-Numpy

After this we would need a data set to train and test our model. From a little searching we have discovered a file including 1001 of the most popular words today, this set would be enough for a wide range of vocabulary.

We need to load the data using pip so we can work with it

Data pre processing:

importing numpy as np
importing pandas as pd

Implement the function process_data which
1) Reads in a corpus
2) Changes everything to lowercase
3) Returns a list of words.

implement get_probs function
to calculate the probability that any word will appear if randomly selected from the dictionary
