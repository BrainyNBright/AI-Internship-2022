Methodology:



Up till now most of our work included gathering the necessary data required to train and test our model.


It's time to integrate this data into a working machine model.


In order to develop the autocorrection model, an algorithm that lays the groundwork for the infrastructure of the cooded program is required.



There exist a wide range of algorithms that can help us effectively reach our goal, such as:


1. Logistic regression 

2. Decision tree

3. Random forest

4. Support Vector Machine

5. K nearest neighbour

6. Naive Bays





Logistic Regression:

-Logistic regression is a statistical analysis method to predict a binary outcome, such as yes or no, based on prior observations of a data set.

- A logistic regression model predicts a dependent data variable by analyzing the relationship between one or more existing independent variables. For example, a logistic regression could be used to predict whether a political candidate will win or lose an election or whether a high school student will be admitted or not to a particular college. These binary outcomes allow straightforward decisions between two alternatives.


Decision  tree:

- Decision tree builds tree branches in a hierarchy approach and each branch can be considered as an if-else statement. The branches develop by partitioning the dataset into subsets based on most important features. Final classification happens at the leaves of the decision tree.

- To elaborate,a decision tree is a graph that uses a branching method to illustrate every possible output for a specific input


Random forest:

- As the name suggest, random forest is a collection of decision trees. It is a common type of ensemble methods which aggregate results from multiple predictors. Random forest additionally utilizes bagging technique that allows each tree trained on a random sampling of original dataset and takes the majority vote from trees. Compared to decision tree, it has better generalization but less interpretable, because of more layers added to the model.


Support Vector Machine:

- Support vector machine finds the best way to classify the data based on the position in relation to a border between positive class and negative class. This border is known as the hyperplane which maximize the distance between data points from different classes. Similar to decision tree and random forest, support vector machine can be used in both classification and regression, SVC (support vector classifier) is for classification problem.

- A support vector machine is also known as a support vector network (SVN).

K nearest neighbour:

What is meant by K nearest neighbor?

- A k-nearest-neighbor is a data classification algorithm that attempts to determine what group a data point is in by looking at the data points around it.

- KNN is a non-parametric method used for classification

Naive Bays:

- Naive Bayes is based on an approach to calculate conditional probability based on prior knowledge, and the naive assumption that each feature is independent to each other.

- The biggest advantage of Naive Bayes is that, while most machine learning algorithms rely on large amount of training data, it performs relatively well even when the training data size is small. Gaussian Naive Bayes is a type of Naive Bayes classifier that follows the normal distribution. 
