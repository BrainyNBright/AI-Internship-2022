Support vector machine:

  Support vector machine (SVM) is a simple algorithm used for classification and regression issues ( mainly used for classification issues ). 
  The whole point of SVM algorithm is to find a hypeline in an N-dimensional space ( N-num of features ) that clasifies the data points so we can 
  put the new data point in the correct place foe the future.
  
  Hyperplanes are decision boundaries that help classify the data points. Data points falling on either side of the hyperplane can be attributed to 
  different classes. Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. 
  Using these support vectors, we maximize the margin of the classifier.   

Random forest:

  Random forest is a frequently used machine learning algorithm that belong to the supervised learning technique wich combines the output of multiple descision
  trees to reach a single result.
  
  Desicion trees can form an ensemble in the random forest algorithm to predict acurate results.
  
  Random forest algorithms have three main hyperparameters, which need to be set before training. These include node size, the number of trees,
  and the number of features sampled

K-Nearest Neighbour:

  The K-Nearest Neighbour algorithm (or K-NN for short) deduces a similarity which connects the new data and available cases. Then it then places the 
  new data into the category that is most similar to the categories at hand.

  When it stores all available data, it classifies a new data point based on the similarity, hence new data can be easily classified into its proper category

  It is a non-parametric algorithm which does not require/create any assumptions or underlying data
  
Naive Bayes algorithm:
  
  its a machine learning algorithm that is based upon probability and is subject to chance variation. This algorithm is highly versatile and is applied is a wide
  selection of classification tasks
  
  The name "naive" is used because it assumes the features that go into the model is independent of each other. That is changing the value of one feature,
  does not directly influence or change the value of any of the other features used in the algorithm.
  
  The great advantage with Naive Bayes algorithm is the ease and speed at which you can code and make predictions with. Thanks to the advantages, it is 
  easily scalable and is teh algorithm of choice in real world applications, which responds to a user's request instantaneously.


----------------------------------------------------------------------------------------------------------------------------------------------------------------------

Exploratory Data Analysis (EDA):

  Once our data has been collected, we need to find the correlations and understand the data provided before making any unnecessary assumptions. Exploratory Data
  Analysis (EDA), mainly used by data scientists to further analyze and investigate a dataset and obtain its main attributes. With this we can then determine
  how best to use the data for our requirements. It also helps us discover the patterns within the dataset, find abnormal data, test our predictions or check
  any assumptions we create.
  
  the process of EDA is broken down into 3 major steps:
  
  1) Data visualization
  2) correlations
  3) Eliminating redundant data
  
  
  Data visualization:
  
  Data visualization is the graphical representation of information and data. It helps people make sense of all the information, or data, generated today. 
  With data visualization, information is represented in graphical form, which allows us to better understand the intricacies a dataset may contain, 
  which we can further use to our benefit.
  Correlations:

  Correlations: 
  
  Correlations can be found by visualizing data in the previous step. This allows us to find the features that are not as correlated, which we can then classify
  as "Redundant data"
  
  Eliminating redundant data:
  
  After finishing the correlations redundant data will be found. This has no effect on training the model and can simply be removed.
  

----------------------------------------------------------------------------------------------------------------------------------------------------------------------
  
  
  
  
  
  
  
  
  
  
  Training our model:
  
  With the data we have collected, we will separate the data into the train and test subset using a 70-30 split (70% for training, 30% for testing).
  These are the variables we will be creating:
   -x_train
   -x_test
   -y_train
   -y_test
   
  The independant variable and target variable (predicted outcome) will be the parameters used in this function.
  
  From this we will import .naive_bayes (our classifier algorithm) from sklearn to start training the model.
  We will use .fit fuction - x_train, y_train (written above) as the parameters.
  
  
  Training accuracy:
  
  Training accuracy is the accuracy of the model when applied to training data, calculated by the number of successes over the total amount of samples in the
  training subset, this can be found by using ".accuracy_score" from sklearn.

