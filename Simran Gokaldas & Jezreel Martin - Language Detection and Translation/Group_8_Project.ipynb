{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Group 8 Project",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7ppCU1DM_UC"
      },
      "outputs": [],
      "source": [
        "!pip install transformers sentencepiece datasets #this is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from google.colab import drive\n",
        "from IPython.display import display\n",
        "from IPython.html import widgets\n",
        "import matplotlib.pyplot as plt          # we have used the matplotlib for our graphs\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "from transformers import AdamW, AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm_notebook\n",
        "sns.set()"
      ],
      "metadata": {
        "id": "EWrAkjt7NWdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "id": "1hpZmnW2PkTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_repo=\"google/mt5-base\"\n",
        "model_path=\"/content/gdrive/My Drive/mt5_translation.pt\"\n",
        "max_seq_len=20"
      ],
      "metadata": {
        "id": "oGiwkET-Qd8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=AutoTokenizer.from_pretrained(model_repo)"
      ],
      "metadata": {
        "id": "HH8kL6oMRRpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=AutoModelForSeq2SeqLM.from_pretrained(model_repo)\n",
        "model=model.cuda"
      ],
      "metadata": {
        "id": "UQjjHJJY9cJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exinput=\"this is just a test üéå\"\n",
        "inputid=tokenizer.encode(exinput,return_tensors=\"pt\")\n",
        "print(\"inputid=\",inputid)\n",
        "tokens= tokenizer.convert_ids_to_tokens(inputid[0])\n",
        "print(\"tokens\",tokens)"
      ],
      "metadata": {
        "id": "_rEVqxQqGkmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(tokenizer.vocab.items(),key=lambda x:x[1])"
      ],
      "metadata": {
        "id": "b9uev8k4I4Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset\n"
      ],
      "metadata": {
        "id": "C9eHsv1ZJqxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=load_dataset(\"alt\")"
      ],
      "metadata": {
        "id": "JQ6OIW0JJpGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=dataset[\"train\"]\n",
        "test_dataset=dataset[\"test\"]"
      ],
      "metadata": {
        "id": "D5qgDAPiKAna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "id": "CNrRceC9KXk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LANG_TOKEN_MAPPING={\n",
        "    \"en\":\"<en>\",\n",
        "    \"ja\":\"<jp>\",\n",
        "    \"zh\":\"<zh>\"\n",
        "}"
      ],
      "metadata": {
        "id": "FnZ7_tLJKl1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s_dict={\"additional_special_tokens\":list(LANG_TOKEN_MAPPING.values())}\n",
        "_special_tokens(special_tokens_dict)\n",
        "token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "2G_0bMchLLOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenid=tokenizer.encode(\n",
        "    exinput,return_tensors=\"pt\",padding=\"max_length\",truncation=True,max_length=max_seq_len)\n",
        "print(tokenid)\n",
        "tokens=tokenizer.convert_ids_to_tokens(tokenid[0])\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHys3yybN6hz",
        "outputId": "43a797a5-2d27-4b18-a6c5-f1a8e9bd674b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[   714,    339,   1627,    259,    262,   2978,    259, 247100,      1,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0]])\n",
            "['‚ñÅthis', '‚ñÅis', '‚ñÅjust', '‚ñÅ', 'a', '‚ñÅtest', '‚ñÅ', 'üéå', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the first function for encoding.\n",
        "def encodeinput(text,targetlang,tokenizer,seq_len,lang_token_map=LANG_TOKEN_MAPPING):\n",
        "  target_lang_token=lang_token_map[targetlang]                   # tokenizing and adding special tokens.\n",
        "  inputid=tokenizer.encode(text=target_lang_token+text,\n",
        "                          return_tensors=\"pt\",\n",
        "                          padding=\"max_length\"\n",
        "                          ,truncation=True\n",
        "                          ,max_length=seq_len)\n",
        "  return inputid[0]\n",
        "\n",
        "# Creating the second function for encoding.\n",
        "def encodetargetstring(text,tokenizer,seq_len,lang_token_map=LANG_TOKEN_MAPPING):\n",
        "  tokenids=tokenizer.encode(\n",
        "      \n",
        "                          text=text,\n",
        "                          return_tensors=\"pt\",\n",
        "                          padding=\"max_length\"\n",
        "                          ,truncation=True\n",
        "                          ,max_length=seq_len)\n",
        "  return tokenids[0]\n",
        "\n",
        "# Creating the third function for encoding.\n",
        "def formattranslationdata(translations,lang_token_map,seq_len=128):     # choosing 2 laguages for input and output.\n",
        "  langs=list(lang_token_map.keys())\n",
        "  input_lang,target_lang=np.random.choice(langs,size=2,replace=False)\n",
        "  input_text=translations[input_lang]\n",
        "  target_text=translations[target_lang]\n",
        "  if input_text is None or target_text is None: \n",
        "    return None\n",
        "  input_token_id=encodeinput(input_text,target_lang,tokenizer,seq_len,lang_token_map)\n",
        "  target_token_id=encodetargetstring(target_text,tokenizer,seq_len,lang_token_map)\n",
        "  return input_token_id,target_token_id\n",
        "  \n",
        "# Creating the fourth function for encoding.\n",
        "def transform_batch(batch,lang_tokens,tokenizer):\n",
        "  inputs=[]\n",
        "  targets=[]\n",
        "  for translation_set in batch[\"translation\"]:\n",
        "    format_data=formattranslationdata(translation_set,tokenizer,max_seq_len)\n",
        "    if format_data is None:\n",
        "      continue \n",
        "    inputid,targetid=format_data\n",
        "    inputs.append(inputid.unsqueeze(0))\n",
        "    targets.append(targetid.unsqueeze(0))\n",
        "\n",
        "  batch_input_ids=torch.cat(inputs).cuda()\n",
        "  \n",
        "  batch_target_ids=torch.cat(targets).cuda()\n",
        "  return batch_input_ids, batch_target_ids\n"
      ],
      "metadata": {
        "id": "FLyMLOuKPWdz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}